{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thebui/Developer/projects/Tell2Design/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser, AutoModelForSeq2SeqLM, EncoderDecoderModel, \\\n",
    "    BertConfig, EncoderDecoderConfig, Trainer\n",
    "from transformers import T5ForConditionalGeneration, T5Config, default_data_collator\n",
    "\n",
    "from arguments import ModelArguments, DataTrainingArguments, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluate, get_avg_results, print_results\n",
    "from utils import get_episode_indices\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('job')\n",
    "parser.add_argument('-c', '--config_file', type=str, default='config.ini', help='configuration file')\n",
    "parser.add_argument('-e', '--eval', action='store_true', default=False, help='run evaluation only')\n",
    "parser.add_argument('--evaluate_checkpoints', action='store_true', default=False,\n",
    "                    help='evaluate intermediate checkpoints instead of the final model')\n",
    "parser.add_argument('--evaluate_last_checkpoint', action='store_true', default=False,\n",
    "                    help='evaluate the last intermediate checkpoint instead of the final model')\n",
    "parser.add_argument('--evaluate_checkpoint_in_dir', type=str, default=None,\n",
    "                    help='evaluate the checkpoint in the given directory')\n",
    "parser.add_argument('-a', '--evaluate_all', action='store_true', default=False,\n",
    "                    help='evaluate intermediate checkpoints together with the final model')\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0, help='which GPU to use for evaluation')\n",
    "parser.add_argument('-v', '--verbose_results', action='store_true', default=False,\n",
    "                    help='print results for each evaluation run')\n",
    "args, remaining_args = parser.parse_known_args()\n",
    "\n",
    "# read config file\n",
    "config = configparser.ConfigParser(allow_no_value=False)\n",
    "config.read(args.config_file)\n",
    "job = args.job\n",
    "assert job in config\n",
    "\n",
    "# set defaults for other arguments\n",
    "defaults = {\n",
    "    'overwrite_output_dir': True,\n",
    "    'overwrite_cache': True,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'learning_rate': 5e-4,\n",
    "    'logging_steps': 1,  # do not log by default = 'logging_steps': 0\n",
    "    'save_steps': 0,  # do not save checkpoints by default\n",
    "}\n",
    "\n",
    "# the config file gives default values for the command line arguments\n",
    "defaults.update(dict(config.items(job)))\n",
    "for key in defaults:\n",
    "    if defaults[key] in ['True', 'False']:\n",
    "        # interpret True/False as boolean\n",
    "        defaults[key] = config.getboolean(job, key)\n",
    "    if defaults[key] == 'None':\n",
    "        # interpret as None\n",
    "        defaults[key] = None\n",
    "\n",
    "if args.eval:\n",
    "    # run evaluation only\n",
    "    defaults['do_train'] = False\n",
    "\n",
    "# parse remaining arguments and divide them into three categories\n",
    "second_parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "second_parser.set_defaults(**defaults)\n",
    "model_args, data_args, training_args = second_parser.parse_args_into_dataclasses(remaining_args)\n",
    "\n",
    "try:\n",
    "    os.mkdir(training_args.output_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# process arguments related to max length\n",
    "if data_args.max_output_seq_length_eval is None:\n",
    "    # defaults first to max_output_seq_length, then max_seq_length_eval, then max_seq_length\n",
    "    data_args.max_output_seq_length_eval = data_args.max_output_seq_length \\\n",
    "                                           or data_args.max_seq_length_eval \\\n",
    "                                           or data_args.max_seq_length\n",
    "\n",
    "if data_args.max_output_seq_length is None:\n",
    "    # defaults to max_seq_length\n",
    "    data_args.max_output_seq_length = data_args.max_seq_length\n",
    "\n",
    "if data_args.max_seq_length_eval is None:\n",
    "    # defaults to max_seq_length\n",
    "    data_args.max_seq_length_eval = data_args.max_seq_length\n",
    "\n",
    "if data_args.chunk_size_eval is None:\n",
    "    # defaults to chunk_size\n",
    "    data_args.chunk_size_eval = data_args.chunk_size\n",
    "\n",
    "if data_args.chunk_overlap_eval is None:\n",
    "    # defaults to chunk overlap\n",
    "    data_args.chunk_overlap_eval = data_args.chunk_overlap\n",
    "\n",
    "# construct name for the output directory\n",
    "# for example: conll04-t5-base-ep200-len256-ratio0-b4-train\n",
    "if data_args.exp:\n",
    "    output_dir = os.path.join(\n",
    "        training_args.output_dir,\n",
    "        f'{args.job}'\n",
    "        f'-{model_args.model_name_or_path.split(\"/\")[-1]}'\n",
    "        f'-{data_args.exp}'\n",
    "        f'-ep{round(training_args.num_train_epochs)}'\n",
    "        f'-len{data_args.max_seq_length}'\n",
    "    )\n",
    "else:\n",
    "    output_dir = os.path.join(\n",
    "        training_args.output_dir,\n",
    "        f'{args.job}'\n",
    "        f'-{model_args.model_name_or_path.split(\"/\")[-1]}'\n",
    "        f'-ep{round(training_args.num_train_epochs)}'\n",
    "        f'-len{data_args.max_seq_length}'\n",
    "    )\n",
    "\n",
    "if data_args.max_output_seq_length != data_args.max_seq_length:\n",
    "    output_dir += f'-{data_args.max_output_seq_length}'\n",
    "\n",
    "if training_args.learning_rate != 5e-4:\n",
    "    output_dir += f'-lr{training_args.learning_rate}'\n",
    "\n",
    "output_dir += f'-b{training_args.per_device_train_batch_size}' \\\n",
    "              f'-{data_args.train_split}'\n",
    "\n",
    "if data_args.chunk_size != 128:\n",
    "    output_dir += f'-chunk{data_args.chunk_size}'\n",
    "if data_args.chunk_overlap != 64:\n",
    "    output_dir += f'-overlap{data_args.chunk_overlap}'\n",
    "\n",
    "if data_args.output_format is not None:\n",
    "    output_dir += f'-{data_args.output_format}'\n",
    "if data_args.input_format is not None:\n",
    "    output_dir += f'-{data_args.input_format}'\n",
    "if data_args.train_subset < 1:\n",
    "    output_dir += f'-size{data_args.train_subset:.2f}'\n",
    "if data_args.output_format_type is not None:\n",
    "    output_dir += f'-{data_args.output_format_type}'\n",
    "if data_args.comment is not None:\n",
    "    output_dir += f'-{data_args.comment}'\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(output_dir, 'logs.log'),\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "# construct file name for the evaluation results\n",
    "evaluation_output_filename = f'results'\n",
    "if data_args.num_beams is not None:\n",
    "    evaluation_output_filename += f'-{data_args.num_beams}beams'\n",
    "if data_args.max_seq_length_eval is not None:\n",
    "    evaluation_output_filename += f'-len{data_args.max_seq_length_eval}'\n",
    "\n",
    "# create model config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    ")\n",
    "\n",
    "# get list of dataset names\n",
    "dataset_names = data_args.datasets.split(',')\n",
    "\n",
    "# construct list of episode indices\n",
    "episode_indices = get_episode_indices(data_args.episodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
